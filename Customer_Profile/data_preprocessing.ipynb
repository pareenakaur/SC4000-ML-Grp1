{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "train=pd.read_csv(\"train.csv\")\n",
    "old_transactions=pd.read_csv(\"cleaned_historical_transactions.csv\")\n",
    "new_transactions=pd.read_csv(\"cleaned_new_merchant_transactions.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No Null Values\n",
    "new_transactions.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No Duplicate Values\n",
    "old_transactions[old_transactions.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train\\n\",train.info())\n",
    "print(\"Old_transactions\\n\",old_transactions.info())\n",
    "print(\"New_transactions\\n\",new_transactions.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_transactions['dataset']='old'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_transactions['dataset']='new'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = pd.concat([old_transactions, new_transactions], axis=0, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to datetime\n",
    "transactions['purchase_date'] = pd.to_datetime(transactions['purchase_date'])\n",
    "\n",
    "#Extract features\n",
    "transactions['purchase_year'] = transactions['purchase_date'].dt.year\n",
    "transactions['purchase_month'] = transactions['purchase_date'].dt.month\n",
    "transactions['purchase_day'] = transactions['purchase_date'].dt.day\n",
    "transactions['purchase_weekday'] = transactions['purchase_date'].dt.weekday\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions=transactions.drop('merchant_id',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions=transactions.drop('city_id',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions=transactions.drop('state_id',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions['card_id'] = transactions['card_id'].astype(str)\n",
    "print(transactions['card_id'].dtype)  #Should now be 'string' or 'object'\n",
    "print(transactions['card_id'].head())  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions.drop('dataset',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_features = transactions.groupby('card_id').agg({\n",
    "    'purchase_amount': ['sum', 'mean', 'max', 'min', 'std'],  # Spending behavior\n",
    "    'installments': ['sum', 'mean'],  # Installment usage\n",
    "    'authorized_flag': ['sum'],  # Total authorized transactions\n",
    "    'category_1': ['sum'],  # Transactions in category_1\n",
    "    'category_2': ['nunique'],  # Unique category_2 values\n",
    "    'category_3': ['nunique'],  # Unique category_3 values\n",
    "    'merchant_category_id': ['nunique'],  # Unique merchant categories\n",
    "    'subsector_id': ['nunique'],  # Unique economic subsectors\n",
    "    'month_lag': ['min', 'max'],  # Recency of transactions\n",
    "    'purchase_date':['min','max'],\n",
    "}).reset_index()\n",
    "agg_features['spending_frequency'] = transactions.groupby('card_id').size()\n",
    "agg_features['avg_spend_per_transaction'] = agg_features['purchase_amount', 'sum'] / agg_features['spending_frequency']\n",
    "agg_features['days_between_first_last'] = (agg_features['purchase_date', 'max'] - agg_features['purchase_date', 'min']).dt.days\n",
    "agg_features['transaction_velocity'] = agg_features['spending_frequency'] / agg_features['days_between_first_last']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_features.rename(columns={'authorized_flag':'num_authorized_transactions'},inplace=True)\n",
    "agg_features.rename(columns={'installments':'num_installment_transactions'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_features.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten column names if they are multi-indexed after aggregation\n",
    "agg_features.columns = ['_'.join(col) if isinstance(col, tuple) else col for col in agg_features.columns]\n",
    "\n",
    "# Reset index to make 'card_id' a column\n",
    "agg_features.reset_index(inplace=True)\n",
    "agg_features.rename(columns={'card_id_':'card_id'},inplace=True)\n",
    "agg_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_features=agg_features.drop(labels=['purchase_date_min','purchase_date_max'],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_features.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Convert first_active_month to datetime\n",
    "train['first_active_month'] = pd.to_datetime(train['first_active_month'])\n",
    "\n",
    "# Compute account age in months\n",
    "current_date = datetime.today()\n",
    "train['account_age_months'] = (current_date.year - train['first_active_month'].dt.year) * 12 + \\\n",
    "                              (current_date.month - train['first_active_month'].dt.month)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.merge(agg_features, on='card_id', how='left')\n",
    "train.fillna(0, inplace=True)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=train.drop(['first_active_month'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# # 1. Select Numerical Features\n",
    "# numerical_features = train.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "# # 3. Create a Min-Max Scaler\n",
    "# scaler = MinMaxScaler()\n",
    "\n",
    "# # 4. Fit and Transform the Numerical Features\n",
    "# train[numerical_features] = scaler.fit_transform(train[numerical_features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X=train.drop(columns=['card_id','target'])\n",
    "Y=train['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training Set Size: {X_train.shape}\")\n",
    "print(f\"Testing Set Size: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "n_folds = 5\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'learning_rate': 0.01,\n",
    "    'num_leaves': 31,\n",
    "    'feature_fraction': 0.8,\n",
    "    'subsample': 0.8\n",
    "}\n",
    "\n",
    "scores = []\n",
    "for train_idx, val_idx in kf.split(X_train):\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "    \n",
    "    lgb_train = lgb.Dataset(X_train_fold, y_train_fold)\n",
    "    lgb_val = lgb.Dataset(X_val_fold, y_val_fold)\n",
    "    \n",
    "    model = lgb.train(params, lgb_train, num_boost_round=1000, \n",
    "                      valid_sets=[lgb_train, lgb_val], \n",
    "                      early_stopping_rounds=50, verbose_eval=100)\n",
    "    \n",
    "    y_pred = model.predict(X_val_fold, num_iteration=model.best_iteration)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val_fold, y_pred))\n",
    "    scores.append(rmse)\n",
    "\n",
    "print(f\"Average RMSE: {np.mean(scores):.4f} (+/- {np.std(scores):.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "params= {\n",
    "    \"objective\":\"reg:squarederror\",\n",
    "    \"learning_rate\":0.01,   # Step size per iteration\n",
    "    \"max_depth\":6,          # Depth of trees\n",
    "}\n",
    "\n",
    "cv_results = xgb.cv(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=1000,\n",
    "    nfold=5,\n",
    "    early_stopping_rounds=50,\n",
    "    metrics=[\"rmse\", \"mae\"],\n",
    "    seed=42,\n",
    "    verbose_eval=100\n",
    ")\n",
    "\n",
    "# Print the best number of rounds and the corresponding scores\n",
    "print(f\"Best number of rounds: {len(cv_results)}\")\n",
    "print(f\"Best RMSE: {cv_results['test-rmse-mean'].min():.4f} (+/- {cv_results['test-rmse-std'].min():.4f})\")\n",
    "print(f\"Best MAE: {cv_results['test-mae-mean'].min():.4f} (+/- {cv_results['test-mae-std'].min():.4f})\")\n",
    "\n",
    "# Train the final model using the best number of rounds\n",
    "best_model = xgb.train(params, dtrain, num_boost_round=len(cv_results))\n",
    "\n",
    "# Make predictions on test data\n",
    "y_pred = best_model.predict(dtest)\n",
    "\n",
    "# Evaluate model performance\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(f'Test RMSE: {rmse:.4f}')\n",
    "print(f'Test MAE: {mae:.4f}')\n",
    "\n",
    "feature_importance = best_model.get_score(importance_type='weight')\n",
    "sorted_importance = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(sorted_importance)), [val[1] for val in sorted_importance])\n",
    "plt.xticks(range(len(sorted_importance)), [val[0] for val in sorted_importance], rotation=90)\n",
    "plt.title('Feature Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import xgboost as xgb\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "# dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# params= {\n",
    "#     \"objective\":\"reg:squarederror\",\n",
    "#     \"n_estimators\":1000,     # Number of trees\n",
    "#     \"learning_rate\":0.01,   # Step size per iteration\n",
    "#     \"max_depth\":6,          # Depth of trees\n",
    "# }\n",
    "\n",
    "# cv_results = xgb.cv(\n",
    "#     params,\n",
    "#     dtrain,\n",
    "#     num_boost_round=1000,\n",
    "#     nfold=5,\n",
    "#     early_stopping_rounds=50,\n",
    "#     metrics=[\"rmse\", \"mae\"],\n",
    "#     seed=42,\n",
    "#     verbose_eval=100\n",
    "# )\n",
    "\n",
    "# # Print the best number of rounds and the corresponding scores\n",
    "# print(f\"Best number of rounds: {len(cv_results)}\")\n",
    "# print(f\"Best RMSE: {cv_results['test-rmse-mean'].min():.4f} (+/- {cv_results['test-rmse-std'].min():.4f})\")\n",
    "# print(f\"Best MAE: {cv_results['test-mae-mean'].min():.4f} (+/- {cv_results['test-mae-std'].min():.4f})\")\n",
    "\n",
    "# # Train the final model using the best number of rounds\n",
    "# best_model = xgb.train(params, dtrain, num_boost_round=len(cv_results))\n",
    "\n",
    "# # Make predictions on test data\n",
    "# y_pred = best_model.predict(dtest)\n",
    "\n",
    "# # Evaluate model performance\n",
    "# rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "# mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "# print(f'Test RMSE: {rmse:.4f}')\n",
    "# print(f'Test MAE: {mae:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train['target'].std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7 (v3.9.7:1016ef3790, Aug 30 2021, 16:39:15) \n[Clang 6.0 (clang-600.0.57)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
