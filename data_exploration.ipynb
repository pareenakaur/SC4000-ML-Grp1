{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import holidays\n",
    "from scipy.stats import entropy\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_trans_df = pd.read_csv(r\"C:\\Users\\Aarushi\\Desktop\\SC4000\\SC4000\\data\\cleaned_data\\cleaned_historical_transactions.csv\")\n",
    "# merch_df = pd.read_csv(r\"C:\\Users\\Aarushi\\Desktop\\SC4000\\SC4000\\data\\cleaned_data\\cleaned_merchants.csv\")\n",
    "new_trans_df = pd.read_csv(r\"C:\\Users\\Aarushi\\Desktop\\SC4000\\SC4000\\data\\cleaned_data\\cleaned_new_merchant_transactions.csv\")\n",
    "train_df = pd.read_csv(r\"C:\\Users\\Aarushi\\Desktop\\SC4000\\SC4000\\data\\cleaned_data\\train.csv\")\n",
    "test_df = pd.read_csv(r\"C:\\Users\\Aarushi\\Desktop\\SC4000\\SC4000\\data\\cleaned_data\\test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "482"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine transactions\n",
    "all_transactions_df = pd.concat([hist_trans_df, new_trans_df])\n",
    "del hist_trans_df, new_trans_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Merge train data with all_transactions based on card_id\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_transactions_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcard_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mleft\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Aarushi\\anaconda3\\envs\\graphrag\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:170\u001b[0m, in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _cross_merge(\n\u001b[0;32m    156\u001b[0m         left_df,\n\u001b[0;32m    157\u001b[0m         right_df,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    167\u001b[0m         copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m    168\u001b[0m     )\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 170\u001b[0m     op \u001b[38;5;241m=\u001b[39m \u001b[43m_MergeOperation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result(copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[1;32mc:\\Users\\Aarushi\\anaconda3\\envs\\graphrag\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:800\u001b[0m, in \u001b[0;36m_MergeOperation.__init__\u001b[1;34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[0;32m    797\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft\u001b[38;5;241m.\u001b[39m_drop_labels_or_levels(left_drop)\n\u001b[0;32m    799\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m right_drop:\n\u001b[1;32m--> 800\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mright\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_labels_or_levels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mright_drop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_require_matching_dtypes(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft_join_keys, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright_join_keys)\n\u001b[0;32m    803\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_tolerance(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft_join_keys)\n",
      "File \u001b[1;32mc:\\Users\\Aarushi\\anaconda3\\envs\\graphrag\\lib\\site-packages\\pandas\\core\\generic.py:1989\u001b[0m, in \u001b[0;36mNDFrame._drop_labels_or_levels\u001b[1;34m(self, keys, axis)\u001b[0m\n\u001b[0;32m   1987\u001b[0m     \u001b[38;5;66;03m# Handle dropping columns labels\u001b[39;00m\n\u001b[0;32m   1988\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels_to_drop:\n\u001b[1;32m-> 1989\u001b[0m         \u001b[43mdropped\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels_to_drop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1990\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1991\u001b[0m     \u001b[38;5;66;03m# Handle dropping column levels\u001b[39;00m\n\u001b[0;32m   1992\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m levels_to_drop:\n",
      "File \u001b[1;32mc:\\Users\\Aarushi\\anaconda3\\envs\\graphrag\\lib\\site-packages\\pandas\\core\\frame.py:5581\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdrop\u001b[39m(\n\u001b[0;32m   5434\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5435\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5442\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5443\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5444\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5445\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5446\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5579\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5580\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5583\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5587\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5588\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5589\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Aarushi\\anaconda3\\envs\\graphrag\\lib\\site-packages\\pandas\\core\\generic.py:4788\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4786\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   4787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4788\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m   4791\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32mc:\\Users\\Aarushi\\anaconda3\\envs\\graphrag\\lib\\site-packages\\pandas\\core\\generic.py:4869\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4866\u001b[0m     new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   4868\u001b[0m bm_axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m-\u001b[39m axis_num \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 4869\u001b[0m new_mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex_indexer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4870\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnew_axis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4871\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4872\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbm_axis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4873\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_dups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   4874\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   4875\u001b[0m \u001b[43m    \u001b[49m\u001b[43monly_slice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43monly_slice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4876\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4877\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(new_mgr, axes\u001b[38;5;241m=\u001b[39mnew_mgr\u001b[38;5;241m.\u001b[39maxes)\n\u001b[0;32m   4878\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Aarushi\\anaconda3\\envs\\graphrag\\lib\\site-packages\\pandas\\core\\internals\\managers.py:680\u001b[0m, in \u001b[0;36mBaseBlockManager.reindex_indexer\u001b[1;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001b[0m\n\u001b[0;32m    677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequested axis not found in manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 680\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_slice_take_blocks_ax0\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    683\u001b[0m \u001b[43m        \u001b[49m\u001b[43monly_slice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43monly_slice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    684\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_na_proxy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_na_proxy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    687\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    688\u001b[0m         blk\u001b[38;5;241m.\u001b[39mtake_nd(\n\u001b[0;32m    689\u001b[0m             indexer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    695\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks\n\u001b[0;32m    696\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\Aarushi\\anaconda3\\envs\\graphrag\\lib\\site-packages\\pandas\\core\\internals\\managers.py:843\u001b[0m, in \u001b[0;36mBaseBlockManager._slice_take_blocks_ax0\u001b[1;34m(self, slice_or_indexer, fill_value, only_slice, use_na_proxy, ref_inplace_op)\u001b[0m\n\u001b[0;32m    841\u001b[0m                     blocks\u001b[38;5;241m.\u001b[39mappend(nb)\n\u001b[0;32m    842\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 843\u001b[0m                 nb \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtaker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_mgr_locs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmgr_locs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    844\u001b[0m                 blocks\u001b[38;5;241m.\u001b[39mappend(nb)\n\u001b[0;32m    846\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m blocks\n",
      "File \u001b[1;32mc:\\Users\\Aarushi\\anaconda3\\envs\\graphrag\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:1307\u001b[0m, in \u001b[0;36mBlock.take_nd\u001b[1;34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[0m\n\u001b[0;32m   1304\u001b[0m     allow_fill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1306\u001b[0m \u001b[38;5;66;03m# Note: algos.take_nd has upcast logic similar to coerce_to_target_dtype\u001b[39;00m\n\u001b[1;32m-> 1307\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43malgos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_fill\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\n\u001b[0;32m   1309\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1311\u001b[0m \u001b[38;5;66;03m# Called from three places in managers, all of which satisfy\u001b[39;00m\n\u001b[0;32m   1312\u001b[0m \u001b[38;5;66;03m#  these assertions\u001b[39;00m\n\u001b[0;32m   1313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ExtensionBlock):\n\u001b[0;32m   1314\u001b[0m     \u001b[38;5;66;03m# NB: in this case, the 'axis' kwarg will be ignored in the\u001b[39;00m\n\u001b[0;32m   1315\u001b[0m     \u001b[38;5;66;03m#  algos.take_nd call above.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Aarushi\\anaconda3\\envs\\graphrag\\lib\\site-packages\\pandas\\core\\array_algos\\take.py:117\u001b[0m, in \u001b[0;36mtake_nd\u001b[1;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mtake(indexer, fill_value\u001b[38;5;241m=\u001b[39mfill_value, allow_fill\u001b[38;5;241m=\u001b[39mallow_fill)\n\u001b[0;32m    116\u001b[0m arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(arr)\n\u001b[1;32m--> 117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_take_nd_ndarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Aarushi\\anaconda3\\envs\\graphrag\\lib\\site-packages\\pandas\\core\\array_algos\\take.py:162\u001b[0m, in \u001b[0;36m_take_nd_ndarray\u001b[1;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m    157\u001b[0m     out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(out_shape, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    159\u001b[0m func \u001b[38;5;241m=\u001b[39m _get_take_nd_function(\n\u001b[0;32m    160\u001b[0m     arr\u001b[38;5;241m.\u001b[39mndim, arr\u001b[38;5;241m.\u001b[39mdtype, out\u001b[38;5;241m.\u001b[39mdtype, axis\u001b[38;5;241m=\u001b[39maxis, mask_info\u001b[38;5;241m=\u001b[39mmask_info\n\u001b[0;32m    161\u001b[0m )\n\u001b[1;32m--> 162\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m flip_order:\n\u001b[0;32m    165\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mT\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Merge train data with all_transactions based on card_id\n",
    "train_df = pd.merge(train_df, all_transactions_df, on='card_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "# Brazilian holidays for the relevant period\n",
    "br_holidays = holidays.BR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_basic_time_features(df):\n",
    "    \"\"\"\n",
    "    Add basic time-based features from purchase_date:\n",
    "    - hour, day, day_of_week, month, year\n",
    "    - is_weekend, is_holiday\n",
    "    - is_start_of_month, is_end_of_month\n",
    "    \"\"\"\n",
    "    df['purchase_datetime'] = pd.to_datetime(df['purchase_date'])\n",
    "    df['hour'] = df['purchase_datetime'].dt.hour\n",
    "    df['day'] = df['purchase_datetime'].dt.day\n",
    "    df['day_of_week'] = df['purchase_datetime'].dt.dayofweek\n",
    "    df['month'] = df['purchase_datetime'].dt.month\n",
    "    df['year'] = df['purchase_datetime'].dt.year\n",
    "    df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "    df['is_holiday'] = df['purchase_datetime'].dt.date.map(lambda x: x in br_holidays).astype(int)\n",
    "    \n",
    "    # Part of month features\n",
    "    df['is_start_of_month'] = (df['day'] <= 5).astype(int)\n",
    "    df['is_end_of_month'] = (df['day'] >= 25).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Add basic time features\n",
    "all_transactions_df = add_basic_time_features(all_transactions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add time difference between transactions\n",
    "def add_time_difference_features(df, card_id_col='card_id'):\n",
    "    \"\"\"Calculate time differences between transactions for each card\"\"\"\n",
    "    # Sort transactions\n",
    "    df = df.sort_values([card_id_col, 'purchase_datetime'])\n",
    "    \n",
    "    # Calculate time between purchases\n",
    "    df['time_since_last'] = df.groupby(card_id_col)['purchase_datetime'].diff()\n",
    "    \n",
    "    # Convert timedelta to hours for easier analysis\n",
    "    df['hours_since_last'] = df['time_since_last'].dt.total_seconds() / 3600\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Add time difference features\n",
    "all_transactions_df = add_time_difference_features(all_transactions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add spending pattern features directly to each transaction\n",
    "def add_spending_pattern_features(df, card_id_col='card_id'):\n",
    "    \"\"\"Add relative spending indicators to each transaction\"\"\"\n",
    "    \n",
    "    # Calculate card-level statistics\n",
    "    card_stats = df.groupby(card_id_col)['purchase_amount'].agg(['mean', 'std']).reset_index()\n",
    "    card_stats.columns = [card_id_col, 'card_mean_amount', 'card_std_amount']\n",
    "    \n",
    "    # Merge back to get card-level stats for each transaction\n",
    "    df = pd.merge(df, card_stats, on=card_id_col, how='left')\n",
    "    \n",
    "    # Calculate z-score of each transaction relative to card's spending pattern\n",
    "    df['amount_zscore'] = (df['purchase_amount'] - df['card_mean_amount']) / df['card_std_amount'].replace(0, 1)\n",
    "    \n",
    "    # Flag unusually large purchases (more than 2 standard deviations)\n",
    "    df['is_large_purchase'] = (df['amount_zscore'] > 2).astype(int)\n",
    "    \n",
    "    # Flag unusually small purchases\n",
    "    df['is_small_purchase'] = (df['amount_zscore'] < -1).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Add spending pattern features\n",
    "all_transactions_df = add_spending_pattern_features(all_transactions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add seasonal features\n",
    "def add_seasonal_features(df, card_id_col='card_id'):\n",
    "    \"\"\"Add seasonal indicators to each transaction\"\"\"\n",
    "    \n",
    "    # Define seasons (for Brazil)\n",
    "    summer_months = [12, 1, 2]  # Brazilian summer\n",
    "    winter_months = [6, 7, 8]   # Brazilian winter\n",
    "    \n",
    "    # Add season flags\n",
    "    df['is_summer'] = df['month'].isin(summer_months).astype(int)\n",
    "    df['is_winter'] = df['month'].isin(winter_months).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Add seasonal features\n",
    "all_transactions_df = add_seasonal_features(all_transactions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add merchant diversity features\n",
    "def add_merchant_diversity_features(df, card_id_col='card_id'):\n",
    "    \"\"\"Add features related to merchant diversity\"\"\"\n",
    "    \n",
    "    # Flag if transaction is with a new merchant (much more efficient than the rolling approach)\n",
    "    # First encounter will be False (not visited before), subsequent encounters will be True\n",
    "    df['merchant_visited_before'] = df.duplicated([card_id_col, 'merchant_category_id']).astype(int)\n",
    "    \n",
    "    # Calculate overall merchant diversity metrics (simpler and faster than rolling)\n",
    "    \n",
    "    # Count total unique merchants per card\n",
    "    merchant_counts = df.groupby(card_id_col)['merchant_category_id'].nunique().reset_index()\n",
    "    merchant_counts.columns = [card_id_col, 'total_unique_merchants']\n",
    "    \n",
    "    # Add total unique merchants to original dataframe\n",
    "    df = pd.merge(df, merchant_counts, on=card_id_col, how='left')\n",
    "    \n",
    "    # Calculate ratio of transactions with repeat merchants\n",
    "    repeat_ratio = df.groupby(card_id_col)['merchant_visited_before'].mean().reset_index()\n",
    "    repeat_ratio.columns = [card_id_col, 'repeat_merchant_ratio']\n",
    "    \n",
    "    # Add repeat merchant ratio to original dataframe\n",
    "    df = pd.merge(df, repeat_ratio, on=card_id_col, how='left')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Add merchant diversity features\n",
    "all_transactions_df = add_merchant_diversity_features(all_transactions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_recency_features(df, card_id_col='card_id'):\n",
    "    \"\"\"Add recency indicators to each transaction\"\"\"\n",
    "    \n",
    "    # Get last transaction date for each card\n",
    "    last_purchase_date = df.groupby(card_id_col)['purchase_datetime'].max().reset_index()\n",
    "    last_purchase_date.columns = [card_id_col, 'last_purchase_date']\n",
    "    \n",
    "    # Merge with transactions\n",
    "    df = pd.merge(df, last_purchase_date, on=card_id_col, how='left')\n",
    "    \n",
    "    # Calculate days since last transaction for each card\n",
    "    df['days_since_last_transaction'] = (df['last_purchase_date'] - df['purchase_datetime']).dt.days\n",
    "    \n",
    "    # Flag recent transactions (last 30 days)\n",
    "    df['is_recent_transaction'] = (df['days_since_last_transaction'] <= 30).astype(int)\n",
    "    \n",
    "    # Flag if transaction is in last 25% of card's transactions\n",
    "    df['transaction_rank'] = df.groupby(card_id_col)['purchase_datetime'].rank(pct=True)\n",
    "    df['is_last_quarter_transaction'] = (df['transaction_rank'] >= 0.75).astype(int)\n",
    "    \n",
    "    # Drop temporary columns\n",
    "    df = df.drop(['last_purchase_date'], axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Add recency features\n",
    "all_transactions_df = add_recency_features(all_transactions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add installment features\n",
    "def add_installment_features(df):\n",
    "    \"\"\"Add features related to installment payments\"\"\"\n",
    "    \n",
    "    # Check if installments column exists\n",
    "    if 'installments' in df.columns:\n",
    "        # Flag if transaction is installment\n",
    "        df['is_installment'] = (df['installments'] > 1).astype(int)\n",
    "        \n",
    "        # Calculate installment amount if applicable\n",
    "        df['installment_amount'] = df['purchase_amount'] / df['installments']\n",
    "        df['installment_amount'] = df['installment_amount'].fillna(df['purchase_amount'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Add installment features\n",
    "all_transactions_df = add_installment_features(all_transactions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add ratio/proportion features\n",
    "def add_ratio_features(df, card_id_col='card_id'):\n",
    "    \"\"\"Add ratio features comparing different transaction types\"\"\"\n",
    "    \n",
    "    # Calculate card-level proportions\n",
    "    card_proportions = df.groupby(card_id_col).agg({\n",
    "        'is_weekend': 'mean',  # Proportion of weekend transactions\n",
    "        'is_holiday': 'mean',  # Proportion of holiday transactions\n",
    "        'is_installment': 'mean',  # Proportion of installment transactions\n",
    "        'is_large_purchase': 'mean',  # Proportion of large purchases\n",
    "        'merchant_visited_before': 'mean',  # Proportion of repeated merchants\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Rename columns\n",
    "    card_proportions.columns = [\n",
    "        card_id_col, \n",
    "        'weekend_trans_ratio', \n",
    "        'holiday_trans_ratio', \n",
    "        'installment_trans_ratio',\n",
    "        'large_purchase_ratio',\n",
    "        'repeat_merchant_ratio'\n",
    "    ]\n",
    "    \n",
    "    # Merge proportions back to main dataframe\n",
    "    df = pd.merge(df, card_proportions, on=card_id_col, how='left')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Add ratio features\n",
    "all_transactions_df = add_ratio_features(all_transactions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add aggregated features\n",
    "def add_aggregated_features(df, card_id_col='card_id'):\n",
    "    \"\"\"Add card-level aggregated features to each transaction\"\"\"\n",
    "    \n",
    "    # Aggregations by card_id\n",
    "    aggs = {\n",
    "        'purchase_amount': ['count', 'sum', 'mean', 'std', 'min', 'max'],\n",
    "        'month': 'nunique',  # Number of unique months with transactions\n",
    "        'merchant_category_id': 'nunique',  # Number of unique merchants\n",
    "        'hours_since_last': ['mean', 'std'],  # Transaction frequency stats\n",
    "        'installments': ['mean', 'max']  # Installment behavior\n",
    "    }\n",
    "    \n",
    "    # Calculate aggregations\n",
    "    card_aggs = df.groupby(card_id_col).agg(aggs)\n",
    "    \n",
    "    # Flatten column names\n",
    "    card_aggs.columns = ['_'.join(col).strip() for col in card_aggs.columns.values]\n",
    "    card_aggs.reset_index(inplace=True)\n",
    "    \n",
    "    # Calculate additional derived features\n",
    "    card_aggs['purchase_amount_range'] = card_aggs['purchase_amount_max'] - card_aggs['purchase_amount_min']\n",
    "    card_aggs['purchase_amount_cv'] = card_aggs['purchase_amount_std'] / card_aggs['purchase_amount_mean']\n",
    "    \n",
    "    # Merge aggregations back to main dataframe\n",
    "    df = pd.merge(df, card_aggs, on=card_id_col, how='left')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Add aggregated features\n",
    "all_transactions_df = add_aggregated_features(all_transactions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final processing\n",
    "# Drop unnecessary columns or create final features\n",
    "all_transactions_df.drop(['time_since_last'], axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "# Scale numerical features if needed\n",
    "# This step depends on your modeling approach - you might want to scale only\n",
    "# when creating the final training dataset rather than scaling the entire dataframe\n",
    "\n",
    "# Export the processed dataframe\n",
    "all_transactions_df.to_csv('all_transactions_with_features.csv', index=False)\n",
    "\n",
    "print(\"Feature engineering completed successfully.\")\n",
    "print(f\"Shape of final dataframe: {all_transactions_df.shape}\")\n",
    "print(f\"Total features: {all_transactions_df.shape[1]}\")\n",
    "print(f\"Sample of new features:\\n{all_transactions_df.iloc[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
